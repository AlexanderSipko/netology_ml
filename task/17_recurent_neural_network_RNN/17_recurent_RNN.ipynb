{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Нейронная маркировка части речи\n",
        "\n",
        "Мы не собираемся решать ту же проблему с маркировкой POS с помощью нейронных сетей.\n",
        "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
        "\n",
        "С точки зрения глубокого обучения, это задача прогнозирования последовательности выходных данных, согласованной с последовательностью входных данных. Есть несколько задач, которые соответствуют этой формулировке:\n",
        "* * Выделение тегов части речи - вспомогательная задача для многих задач NLP\n",
        "* Распознавание именованных объектов - для чат-ботов и веб-сканеров\n",
        "* Прогнозирование структуры белка - для биоинформатики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\avsip\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     C:\\Users\\avsip\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
        "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
        "\n",
        "data = np.array([[(word.lower(), tag) for word, tag in sentence] for sentence in data], dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# тестирование модели\n",
        "def predict_tags(model, word_to_id, test_sentence):\n",
        "    test_sequence = [word_to_id.get(word.lower(), 0) for word in test_sentence]\n",
        "    predicted_tags_probabilities = model.predict(np.array([test_sequence]))\n",
        "    predicted_tags = np.argmax(predicted_tags_probabilities, axis=-1)[0]\n",
        "    predicted_tags_text = [all_tags[tag_id] for tag_id in predicted_tags]\n",
        "    return predicted_tags_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data,test_size=0.25,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "def draw(sentence):\n",
        "    words,tags = zip(*sentence)\n",
        "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
        "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
        "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
        "\n",
        "draw(data[11])\n",
        "draw(data[10])\n",
        "draw(data[7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Построение словарей\n",
        "\n",
        "Как и раньше, мы должны построить отображение от токенов к целочисленным идентификаторам. На этот раз наша модель работает на уровне слов, обрабатывая одно слово за шаг RNN. Это означает, что нам придется иметь дело с гораздо большим словарным запасом.\n",
        "К счастью для нас, мы получаем только эти слова в качестве входных данных, т.е. нам не нужно их предсказывать. Это означает, что мы можем бесплатно получить большой словарный запас, используя встраивания слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coverage = 0.92876\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter()\n",
        "for sentence in data:\n",
        "    words,tags = zip(*sentence)\n",
        "    word_counts.update(words)\n",
        "\n",
        "all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0])\n",
        "\n",
        "print(\"Coverage = %.5f\" % (float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "word_to_id = defaultdict(lambda:1, { word: i for i, word in enumerate(all_words) })\n",
        "tag_to_id = { tag: i for i, tag in enumerate(all_tags)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "преобразуйте слова и теги в матрицу фиксированного размера"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_matrix(lines, token_to_id, max_len=None, pad=0, dtype='int32', time_major=False):\n",
        "    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\"\"\"\n",
        "\n",
        "    max_len = max_len or max(map(len,lines))\n",
        "    matrix = np.empty([len(lines), max_len],dtype)\n",
        "    matrix.fill(pad)\n",
        "\n",
        "    for i in range(len(lines)):\n",
        "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
        "        matrix[i,:len(line_ix)] = line_ix\n",
        "\n",
        "    return matrix.T if time_major else matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word ids:\n",
            "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
            "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
            "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
            "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
            "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
            "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
            " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
            "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n",
            "Tag ids:\n",
            "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
            "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0]\n",
            " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
            "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
            "   6  3  2 13  7]\n",
            " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0]]\n"
          ]
        }
      ],
      "source": [
        "batch_words, batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
        "\n",
        "print(\"Word ids:\")\n",
        "print(to_matrix(batch_words, word_to_id))\n",
        "print(\"Tag ids:\")\n",
        "print(to_matrix(batch_tags, tag_to_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Построить модель\n",
        "\n",
        "В отличие от нашей предыдущей лабораторной работы, на этот раз мы сосредоточимся на высокоуровневом интерфейсе keras для рекуррентных нейронных сетей.\n",
        "Он настолько прост, насколько это возможно с помощью RUN, хотя и несколько ограничен для сложных задач, таких как seq2 seq.\n",
        "По умолчанию все RNN keras применяются ко всей последовательности входных данных и выдают последовательность скрытых состояний `(return_sequences=True`\n",
        "или только последнее скрытое состояние `(return_sequences=False)`. Все повторение происходит под капотом.\n",
        "В верхней части нашей модели нам нужно нанести плотный слой на каждый временной шаг независимо.\n",
        "На данный момент по умолчанию используется keras.слои.Dense будет применяться один раз ко всем объединенным временным шагам.\n",
        "Мы используем __keras.слои.TimeDistributed__ для изменения плотного слоя таким образом, чтобы он применялся как по пакетной, так и по временной осям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras.layers as L\n",
        "import tensorflow as tf\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None],dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words),50))\n",
        "model.add(L.SimpleRNN(64,return_sequences=True))\n",
        "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n",
        "# исправление Warning ()\n",
        "# tf_loss_function = tf.compat.v1.losses.sparse_softmax_cross_entropy\n",
        "# tf_default_graph = tf.compat.v1.get_default_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__ Обучение:__ в этом случае мы не хотим заранее подготавливать весь набор обучающих данных. Основная причина заключается в том, что длина каждого пакета зависит от максимальной длины предложения в пакете. Это оставляет нам два варианта: использовать пользовательский обучающий код, как на предыдущем семинаре, или использовать генераторы.\n",
        "\n",
        "Модели Keras имеют метод __`model.fit_generator`__, который принимает генератор python, выдающий по одному пакету за раз. Но сначала нам нужно реализовать такой генератор:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "BATCH_SIZE=32\n",
        "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
        "    assert isinstance(sentences,np.ndarray),\"Make sure sentences is q numpy array\"\n",
        "\n",
        "    while True:\n",
        "        indices = np.random.permutation(np.arange(len(sentences)))\n",
        "        for start in range(0,len(indices)-1,batch_size):\n",
        "            batch_indices = indices[start:start+batch_size]\n",
        "            batch_words,batch_tags = [],[]\n",
        "            for sent in sentences[batch_indices]:\n",
        "                words,tags = zip(*sent)\n",
        "                batch_words.append(words)\n",
        "                batch_tags.append(tags)\n",
        "\n",
        "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
        "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
        "\n",
        "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
        "            yield batch_words,batch_tags_1hot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Callbacks:__ Еще одна вещь, которая нам нужна, - это измерение производительности модели. Сложность заключается в том, чтобы не подсчитывать точность после окончания предложения (при заполнении) и убедиться, что мы подсчитываем все данные проверки ровно один раз.\n",
        "\n",
        "Хотя убедить Keras сделать все это не невозможно, мы также можем написать наш собственный обратный вызов, который это делает.\n",
        "Обратные вызовы Keras позволяют вам писать пользовательский код, который будет запускаться один раз в каждую эпоху или в каждом мини-пакете. Мы определим его с помощью обратного вызова Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_test_accuracy(model):\n",
        "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
        "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
        "\n",
        "    #predict tag probabilities of shape [batch,time,n_tags]\n",
        "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
        "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
        "\n",
        "    #compute accurary excluding padding\n",
        "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
        "    denominator = np.sum(test_words != 0)\n",
        "    return float(numerator)/denominator\n",
        "\n",
        "\n",
        "class EvaluateAccuracy(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs=None):\n",
        "        sys.stdout.flush()\n",
        "        print(\"\\nMeasuring validation accuracy...\")\n",
        "        acc = compute_test_accuracy(self.model)\n",
        "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
        "        sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\avsip\\AppData\\Local\\Temp\\ipykernel_15672\\1670692242.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1344/1343 [==============================] - ETA: 0s - loss: 0.2433\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 5s 10ms/step\n",
            "\n",
            "Validation accuracy: 0.94084\n",
            "\n",
            "1343/1343 [==============================] - 26s 18ms/step - loss: 0.2433\n",
            "Epoch 2/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0578\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 5s 10ms/step\n",
            "\n",
            "Validation accuracy: 0.94508\n",
            "\n",
            "1343/1343 [==============================] - 23s 17ms/step - loss: 0.0578\n",
            "Epoch 3/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0511\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 5s 10ms/step\n",
            "\n",
            "Validation accuracy: 0.94680\n",
            "\n",
            "1343/1343 [==============================] - 25s 18ms/step - loss: 0.0511\n",
            "Epoch 4/5\n",
            "1342/1343 [============================>.] - ETA: 0s - loss: 0.0469\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 5s 11ms/step\n",
            "\n",
            "Validation accuracy: 0.94561\n",
            "\n",
            "1343/1343 [==============================] - 24s 18ms/step - loss: 0.0469\n",
            "Epoch 5/5\n",
            "1341/1343 [============================>.] - ETA: 0s - loss: 0.0431\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 5s 11ms/step\n",
            "\n",
            "Validation accuracy: 0.94618\n",
            "\n",
            "1343/1343 [==============================] - 25s 19ms/step - loss: 0.0432\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2017c2e2980>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile('adam','categorical_crossentropy')\n",
        "\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 5s 11ms/step\n",
            "Final accuracy: 0.94618\n"
          ]
        }
      ],
      "source": [
        "acc = compute_test_accuracy(model)\n",
        "print(\"Final accuracy: %.5f\"%acc)\n",
        "\n",
        "assert acc>0.94, \"Keras has gone on a rampage again, please contact course staff.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 138ms/step\n",
            "Input Sentence: ['This', 'is', 'a', 'test', 'sentence']\n",
            "Predicted Tags: ['DET', 'VERB', 'DET', 'NOUN', 'NOUN']\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "predicted_tags_result = predict_tags(model, word_to_id, test_sentence)\n",
        "\n",
        "print(\"Input Sentence:\", test_sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Двунаправленный анализ\n",
        "\n",
        "Поскольку мы анализируем полную последовательность, для нас законно просматривать будущие данные.\n",
        "Простой способ добиться этого - работать в обоих направлениях одновременно, создавая __двунаправленный RNN__.\n",
        "В Keras вы можете добиться этого как вручную (используя два элемента и объединение), так и с помощью __`keras.слои.Bidirectional`__.\n",
        "Это работает так же, как \"Распределенное по времени\", которое мы видели ранее: вы оборачиваете его вокруг повторяющегося слоя (SimpleRNN сейчас и LSTM / GRU позже), и это фактически создает два слоя под капотом.\n",
        "Ваша первая задача - использовать такой слой в нашем POS-теггере."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None], dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words), 50))\n",
        "model.add(L.Bidirectional(L.SimpleRNN(64, return_sequences=True)))\n",
        "stepwise_dense = L.Dense(len(all_tags), activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\avsip\\AppData\\Local\\Temp\\ipykernel_15672\\1670692242.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1344/1343 [==============================] - ETA: 0s - loss: 0.2044\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 9s 20ms/step\n",
            "\n",
            "Validation accuracy: 0.95564\n",
            "\n",
            "1343/1343 [==============================] - 39s 28ms/step - loss: 0.2044\n",
            "Epoch 2/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0430\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 10s 22ms/step\n",
            "\n",
            "Validation accuracy: 0.96092\n",
            "\n",
            "1343/1343 [==============================] - 41s 31ms/step - loss: 0.0430\n",
            "Epoch 3/5\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0352\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 10s 22ms/step\n",
            "\n",
            "Validation accuracy: 0.96267\n",
            "\n",
            "1343/1343 [==============================] - 40s 30ms/step - loss: 0.0352\n",
            "Epoch 4/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0297\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 10s 22ms/step\n",
            "\n",
            "Validation accuracy: 0.96238\n",
            "\n",
            "1343/1343 [==============================] - 41s 31ms/step - loss: 0.0297\n",
            "Epoch 5/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0248\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 10s 21ms/step\n",
            "\n",
            "Validation accuracy: 0.96187\n",
            "\n",
            "1343/1343 [==============================] - 40s 30ms/step - loss: 0.0248\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x201e460c640>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile('adam','categorical_crossentropy')\n",
        "\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 9s 21ms/step\n",
            "\n",
            "Final accuracy: 0.96187\n",
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
        "\n",
        "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 249ms/step\n",
            "Input Sentence: ['This', 'is', 'a', 'test', 'sentence']\n",
            "Predicted Tags: ['DET', 'VERB', 'DET', 'ADJ', 'NOUN']\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "predicted_tags_result = predict_tags(model, word_to_id, test_sentence)\n",
        "\n",
        "print(\"Input Sentence:\", test_sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создайте **хотя бы один эксперимент** из приведенного ниже списка вы можете выбрать наиболее интересные и многообещающие варианты повышения производительности двунаправленного LSTM:\n",
        "\n",
        "* __Go beyond SimpleRNN__: there's `keras.layers.LSTM` and `keras.layers.GRU`\n",
        "  * Если вы хотите использовать пользовательскую рекуррентную ячейку, прочитайте [это](https://keras.io/layers/recurrent/#rnn)\n",
        "  * Вы также можете использовать одномерные свертки (`keras.слои.Conv1D`). Они часто так же хороши, как и повторяющиеся слои, но требуют меньшего переобучения.\n",
        "* __Stack more layers__: если в этом курсе и есть общий мотив, то он касается укладки слоев\n",
        "  * Вы можете просто добавить слои recurrent и `1dconv` один поверх другого, и `keras` это поймет\n",
        "  * Просто помните, что большим сетям может потребоваться больше эпох для обучения\n",
        "* __Regularization__: вы можете применять отсевы как обычно, но также и специфичным для RNN способом\n",
        "  * `keras.laers.Dropout` работает между слоями RNN\n",
        "  * Повторяющиеся слои также имеют параметр `recurrent_dropout`\n",
        "*  __Gradient clipping__: Если ваше обучение не так стабильно, как вам хотелось бы, установите `clipnorm` в вашем оптимизаторе.\n",
        "   * То есть, это хорошая идея - следить за своей кривой потерь при каждом минибатче. Попробуйте обратный вызов `tensorboard` или что-то подобное."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Go beyond SimpleRNN__ add `L.Bidirectional(L.LSTM)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\avsip\\AppData\\Local\\Temp\\ipykernel_15672\\3956650167.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1344/1343 [==============================] - ETA: 0s - loss: 0.2474\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 25s 54ms/step\n",
            "\n",
            "Validation accuracy: 0.95396\n",
            "\n",
            "1343/1343 [==============================] - 101s 72ms/step - loss: 0.2474\n",
            "Epoch 2/5\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0456\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 23s 51ms/step\n",
            "\n",
            "Validation accuracy: 0.96043\n",
            "\n",
            "1343/1343 [==============================] - 93s 69ms/step - loss: 0.0456\n",
            "Epoch 3/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0372\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 22s 50ms/step\n",
            "\n",
            "Validation accuracy: 0.96315\n",
            "\n",
            "1343/1343 [==============================] - 90s 67ms/step - loss: 0.0372\n",
            "Epoch 4/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0326\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 23s 52ms/step\n",
            "\n",
            "Validation accuracy: 0.96426\n",
            "\n",
            "1343/1343 [==============================] - 90s 67ms/step - loss: 0.0326\n",
            "Epoch 5/5\n",
            "1343/1343 [============================>.] - ETA: 0s - loss: 0.0289\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 23s 51ms/step\n",
            "\n",
            "Validation accuracy: 0.96493\n",
            "\n",
            "1343/1343 [==============================] - 92s 68ms/step - loss: 0.0290\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x201e50e9cc0>"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None], dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words), 50))\n",
        "\n",
        "# Bidirectional LSTM\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True)))\n",
        "stepwise_dense = L.Dense(len(all_tags), activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n",
        "model.compile('adam','categorical_crossentropy')\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 23s 51ms/step\n",
            "\n",
            "Final accuracy Bidirectional LSTM: 0.96493\n"
          ]
        }
      ],
      "source": [
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy Bidirectional LSTM: %.5f\"%acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 591ms/step\n",
            "Input Sentence: ['This', 'is', 'a', 'test', 'sentence']\n",
            "Predicted Tags: ['DET', 'VERB', 'DET', 'NOUN', 'NOUN']\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "predicted_tags_result = predict_tags(model, word_to_id, test_sentence)\n",
        "\n",
        "print(\"Input Sentence:\", test_sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Stack more layers__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\avsip\\AppData\\Local\\Temp\\ipykernel_15672\\2297888989.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1344/1343 [==============================] - ETA: 0s - loss: 0.2625\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 27s 59ms/step\n",
            "\n",
            "Validation accuracy: 0.94954\n",
            "\n",
            "1343/1343 [==============================] - 200s 141ms/step - loss: 0.2625\n",
            "Epoch 2/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0602\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 25s 56ms/step\n",
            "\n",
            "Validation accuracy: 0.95637\n",
            "\n",
            "1343/1343 [==============================] - 175s 130ms/step - loss: 0.0602\n",
            "Epoch 3/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0498\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 18s 41ms/step\n",
            "\n",
            "Validation accuracy: 0.96045\n",
            "\n",
            "1343/1343 [==============================] - 170s 126ms/step - loss: 0.0498\n",
            "Epoch 4/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0436\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.96245\n",
            "\n",
            "1343/1343 [==============================] - 142s 106ms/step - loss: 0.0436\n",
            "Epoch 5/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0390\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.96381\n",
            "\n",
            "1343/1343 [==============================] - 144s 107ms/step - loss: 0.0390\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x201e59b95a0>"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None], dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words), 50))\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(L.Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "stepwise_dense = L.Dense(len(all_tags), activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n",
        "model.compile('adam','categorical_crossentropy')\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 18s 38ms/step\n",
            "\n",
            "Final accuracy Bidirectional LSTM and Bidirectional LSTM: 0.96381\n"
          ]
        }
      ],
      "source": [
        "model.compile('adam', 'categorical_crossentropy')\n",
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy Bidirectional LSTM and Bidirectional LSTM: %.5f\"%acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 551ms/step\n",
            "Input Sentence: ['This', 'is', 'a', 'test', 'sentence']\n",
            "Predicted Tags: ['DET', 'VERB', 'DET', 'NOUN', 'NOUN']\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "predicted_tags_result = predict_tags(model, word_to_id, test_sentence)\n",
        "\n",
        "print(\"Input Sentence:\", test_sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Regularization__ `Dropout`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\avsip\\AppData\\Local\\Temp\\ipykernel_15672\\1620620932.py:13: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1344/1343 [==============================] - ETA: 0s - loss: 0.3130\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 18s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.94708\n",
            "\n",
            "1343/1343 [==============================] - 153s 108ms/step - loss: 0.3130\n",
            "Epoch 2/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0663\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 39ms/step\n",
            "\n",
            "Validation accuracy: 0.95597\n",
            "\n",
            "1343/1343 [==============================] - 148s 110ms/step - loss: 0.0663\n",
            "Epoch 3/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0536\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.95948\n",
            "\n",
            "1343/1343 [==============================] - 148s 110ms/step - loss: 0.0536\n",
            "Epoch 4/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0473\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.96138\n",
            "\n",
            "1343/1343 [==============================] - 157s 117ms/step - loss: 0.0473\n",
            "Epoch 5/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0430\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 39ms/step\n",
            "\n",
            "Validation accuracy: 0.96374\n",
            "\n",
            "1343/1343 [==============================] - 153s 114ms/step - loss: 0.0430\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x201ff9ac880>"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None], dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words), 50))\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(L.Dropout(0.5))\n",
        "model.add(L.Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "stepwise_dense = L.Dense(len(all_tags), activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n",
        "model.compile('adam','categorical_crossentropy')\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 18s 39ms/step\n",
            "\n",
            "Final accuracy Regularization (Dropout): 0.96374\n"
          ]
        }
      ],
      "source": [
        "model.compile('adam', 'categorical_crossentropy')\n",
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy Regularization (Dropout): %.5f\"%acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 569ms/step\n",
            "Input Sentence: ['This', 'is', 'a', 'test', 'sentence']\n",
            "Predicted Tags: ['DET', 'VERB', 'DET', 'NOUN', 'NOUN']\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "predicted_tags_result = predict_tags(model, word_to_id, test_sentence)\n",
        "\n",
        "print(\"Input Sentence:\", test_sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Gradient clipping__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\avsip\\AppData\\Local\\Temp\\ipykernel_15672\\1404121170.py:13: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1344/1343 [==============================] - ETA: 0s - loss: 0.3126\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 18s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.94626\n",
            "\n",
            "1343/1343 [==============================] - 156s 111ms/step - loss: 0.3126\n",
            "Epoch 2/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0691\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 38ms/step\n",
            "\n",
            "Validation accuracy: 0.95516\n",
            "\n",
            "1343/1343 [==============================] - 149s 111ms/step - loss: 0.0691\n",
            "Epoch 3/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0546\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 18s 39ms/step\n",
            "\n",
            "Validation accuracy: 0.95796\n",
            "\n",
            "1343/1343 [==============================] - 153s 114ms/step - loss: 0.0546\n",
            "Epoch 4/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0480\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 18s 39ms/step\n",
            "\n",
            "Validation accuracy: 0.96079\n",
            "\n",
            "1343/1343 [==============================] - 154s 114ms/step - loss: 0.0480\n",
            "Epoch 5/5\n",
            "1344/1343 [==============================] - ETA: 0s - loss: 0.0429\n",
            "Measuring validation accuracy...\n",
            "448/448 [==============================] - 17s 39ms/step\n",
            "\n",
            "Validation accuracy: 0.96343\n",
            "\n",
            "1343/1343 [==============================] - 155s 115ms/step - loss: 0.0429\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x20210617a30>"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(L.InputLayer([None], dtype='int32'))\n",
        "model.add(L.Embedding(len(all_words), 50))\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(L.Bidirectional(L.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(L.Dropout(0.5))\n",
        "model.add(L.Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "stepwise_dense = L.Dense(len(all_tags), activation='softmax')\n",
        "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
        "model.add(stepwise_dense)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.0), loss='categorical_crossentropy')\n",
        "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
        "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 18s 39ms/step\n",
            "\n",
            "Final accuracy Regularization (Dropout) and clipnorm: 0.96343\n"
          ]
        }
      ],
      "source": [
        "model.compile('adam', 'categorical_crossentropy')\n",
        "acc = compute_test_accuracy(model)\n",
        "print(\"\\nFinal accuracy Regularization (Dropout) and clipnorm: %.5f\"%acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 548ms/step\n",
            "Input Sentence: ['This', 'is', 'a', 'test', 'sentence']\n",
            "Predicted Tags: ['DET', 'VERB', 'DET', 'NOUN', 'NOUN']\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]\n",
        "predicted_tags_result = predict_tags(model, word_to_id, test_sentence)\n",
        "\n",
        "print(\"Input Sentence:\", test_sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Вывод\n",
        "\n",
        "`SimpleRNN`:\n",
        " + точность: `94.618%`\n",
        " + Предсказанные теги: `['DET', 'VERB', 'DET', 'NOUN', 'NOUN']`\n",
        "\n",
        "`Bidirectional SimpleRNN`:\n",
        " + точность: `96.187%`\n",
        " + Предсказанные теги: `['DET', 'VERB', 'DET', 'ADJ', 'NOUN']`\n",
        " + Введение двунаправленности улучшило точность до `96.187%`.\n",
        "  \n",
        "`Bidirectional LSTM:` &#9989;\n",
        " + точность: `96.493%`\n",
        " + Предсказанные теги: `['DET', 'VERB', 'DET', 'NOUN', 'NOUN']`\n",
        " + Использование `Bidirectional LSTM` дополнительно увеличило точность до `96.493%`.\n",
        " + Предсказанные теги, остались схожими с `Bidirectional SimpleRNN`.\n",
        "\n",
        "Двойной `Bidirectional LSTM`:\n",
        " + точность: `96.381%`\n",
        " + Предсказанные теги: `['DET', 'VERB', 'DET', 'NOUN', 'NOUN']`\n",
        " + Использование двух слоев `Bidirectional LSTM` снизило точность `96.381%`.\n",
        "\n",
        "`Bidirectional LSTM с Dropout (0.5)`:\n",
        " + точность: `96.374%`\n",
        " + Предсказанные теги: `['DET', 'VERB', 'DET', 'NOUN', 'NOUN']`\n",
        " + `dropout` `0.5` не существенно изменило точность (`96.374%`)\n",
        "\n",
        "`Bidirectional LSTM с Dropout (0.5) и clipnorm (1.0):`\n",
        " + точность: `96.343%`\n",
        " + Предсказанные теги: `['DET', 'VERB', 'DET', 'NOUN', 'NOUN']`\n",
        " + Применение как `dropout`, так и `clipnorm` в оптимизаторе снизило точность до `96.343%`\n",
        "\n",
        "`Bidirectional LSTM` &#9989; лучший выбором для данной задачи, поскольку он превосходит `SimpleRNN`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
